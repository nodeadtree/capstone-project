{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing we need to do is manage our imports. Without importing these things, we cannot use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import letp\n",
    "import sklearn.metrics as me\n",
    "import sklearn.datasets as d\n",
    "import sklearn.neighbors as nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some measurements, and add them to the appropriate dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record(measurement):\n",
    "    return measurement\n",
    "\n",
    "def confusion_matrix(measurement):\n",
    "    conf_matrix = me.confusion_matrix(measurement[0],measurement[1])\n",
    "    return conf_matrix\n",
    "\n",
    "def precision(measurement):\n",
    "    precision_score = me.precision_score(measurement[0],measurement[1], average='samples')\n",
    "    return precision_score\n",
    "\n",
    "def recall(measurement):\n",
    "    recall_score = me.recall_score(measurement[0],measurement[1])\n",
    "    return recall_score\n",
    "\n",
    "def f1(measurement):\n",
    "    f1_score = me.f1_score(measurement[0],measurement[1])\n",
    "    return f1_score\n",
    "\n",
    "def accuracy(measurement):\n",
    "    correct = 0\n",
    "    total = len(measurement[0])\n",
    "    for i, j in zip(*measurement):\n",
    "        if i == j:\n",
    "            correct += 1\n",
    "    return correct / total\n",
    "\n",
    "analysis_functions = {\n",
    "        \"conf_matrix\" : confusion_matrix,\n",
    "        \"accuracy\" : accuracy\n",
    "        #\"precision\" : precision,\n",
    "        #\"recall\" : recall,\n",
    "        #\"f1\" : f1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat, next thing we need to do is define the measurements we want to use our analysis functions on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = {\n",
    "        \"pred_true_values\" : ['conf_matrix', 'accuracy']#, 'precision', 'recall', 'f1']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make the analyzer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = letp.Analyzer(measurements, analysis_functions, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we bring in the dataset, we'll import it from sklearn for brevity's sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8)\n"
     ]
    }
   ],
   "source": [
    "data = d.load_digits()\n",
    "print(np.shape(data['images'][0]))\n",
    "input_data = {'X': np.array([i.flatten() for i in data['images']]), 'Y': data['target']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a partitioner. The partitioner will break the dataset into training and testing components. We'll use one training and testing set for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partitioner(data):\n",
    "    split = int(len(data['Y'])*.8)\n",
    "    output = { \n",
    "        'train_data': data['X'][:split],\n",
    "        'train_labels': data['Y'][:split],\n",
    "        'test_data': data['X'][split:],\n",
    "        'test_labels': data['Y'][split:]\n",
    "    }\n",
    "    yield output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the data handler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = letp.DataHandler(input_data, partitioner=partitioner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a step function. This is responsible for each experimental step, and is expected to be run on every iteration of the partitioner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(data):\n",
    "    model= nn.KNeighborsClassifier() \n",
    "    model.fit(data['train_data'], data['train_labels'])\n",
    "    model.fit(data['train_data'], data['train_labels'])\n",
    "    output_labels = model.predict(data['test_data'])\n",
    "    yield ('pred_true_values', (output_labels, data['test_labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instantiate a cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle = letp.Cycle(analyzer, data_handler, step, name='K Nearest Neighbors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that a cycle has been created, it must still be run, which we do with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pred_true_values-0': (array([2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7,\n",
      "       7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6,\n",
      "       6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4,\n",
      "       6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5,\n",
      "       4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 9, 0, 8, 9, 8, 0, 1,\n",
      "       2, 3, 4, 5, 6, 7, 1, 9, 0, 1, 2, 3, 4, 5, 6, 9, 0, 1, 2, 3, 4, 5,\n",
      "       6, 7, 8, 9, 4, 9, 5, 5, 6, 5, 0, 9, 8, 5, 8, 4, 1, 7, 7, 3, 5, 1,\n",
      "       0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 7, 8, 4, 6, 6, 6, 7, 9,\n",
      "       1, 5, 0, 9, 5, 2, 8, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1,\n",
      "       7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 2, 5,\n",
      "       7, 3, 5, 8, 4, 5, 0, 8, 9, 7, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1,\n",
      "       2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5,\n",
      "       6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0,\n",
      "       1, 2, 6, 3, 2, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2,\n",
      "       0, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 5, 1,\n",
      "       4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4,\n",
      "       8, 1, 4, 9, 0, 8, 9, 8]), array([2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7,\n",
      "       7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6,\n",
      "       6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4,\n",
      "       6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5,\n",
      "       4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 9, 0, 8, 9, 8, 0, 1,\n",
      "       2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 9, 0, 1, 2, 3, 4, 5,\n",
      "       6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1,\n",
      "       0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9,\n",
      "       1, 5, 0, 9, 5, 2, 8, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1,\n",
      "       7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 2, 5,\n",
      "       7, 9, 5, 4, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1,\n",
      "       2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5,\n",
      "       6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0,\n",
      "       1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2,\n",
      "       0, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1,\n",
      "       4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4,\n",
      "       8, 8, 4, 9, 0, 8, 9, 8])), 'pred_true_values-0-analysis': {'conf_matrix': array([[34,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0, 36,  0,  0,  0,  0,  0,  0,  2,  0],\n",
      "       [ 0,  0, 35,  1,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0, 33,  0,  0,  0,  0,  0,  1],\n",
      "       [ 1,  0,  0,  0, 35,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  1,  0, 37,  0,  0,  0,  2],\n",
      "       [ 0,  0,  0,  0,  0,  0, 37,  0,  0,  0],\n",
      "       [ 0,  0,  0,  1,  1,  0,  0, 36,  1,  0],\n",
      "       [ 0,  0,  0,  1,  1,  0,  0,  0, 30,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0, 34]]), 'accuracy': 0.9638888888888889}}\n"
     ]
    }
   ],
   "source": [
    "results = cycle.run()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this output, it's clear that the measurements' analyses are exactly where they should be. We can specifically look\n",
    "at the outputs by selecting the appropriate dictionaries. Let's pull up only the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9638888888888889\n"
     ]
    }
   ],
   "source": [
    "print(results['pred_true_values-0-analysis']['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, there's some outputs. This is good, but more would be better. Let's write a function that creates cycle functions, and use that to test a bunch of classifiers at once. The first thing we should do though is import some new classifiers.\n",
    "\n",
    "$det(\\lambda A-B)=det(A)det(A^{-1})det(\\lambda A-B)=det(A)det(A^{-1}(\\lambda A-B))=det(A)det(\\lambda I-A^{-1}B)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble as ens\n",
    "import sklearn.discriminant_analysis as da\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.naive_bayes as nb\n",
    "\n",
    "\n",
    "def forstner_metric(a, b):\n",
    "    a = a.reshape((8,8))\n",
    "    b = b.reshape((8,8))\n",
    "    c = np.linalg.svd(a)\n",
    "    d = np.linalg.svd(b)\n",
    "    c_covar = np.matmul(c[0].T, np.diag(np.reciprocal(c[1])))\n",
    "    d_covar = np.matmul(c_covar,np.matmul(d[0], np.diag(d[1])))\n",
    "    d_covar[~np.isfinite(d_covar)] = 0\n",
    "    print(np.linalg.svd(d_covar)[1])\n",
    "    forstner = np.sqrt(np.sum(np.log(np.linalg.svd(d_covar)[1]**2)**2))\n",
    "    print(forstner)\n",
    "    return forstner\n",
    "\n",
    "model_list = [ens.RandomForestClassifier,\n",
    "              da.LinearDiscriminantAnalysis,\n",
    "              nb.GaussianNB,\n",
    "              nn.KNeighborsClassifier,\n",
    "              lm.LogisticRegression,\n",
    "              lambda : nn.KNeighborsClassifier(metric=forstner_metric)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to define the cycle generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_generator(models, analyzer, data_handler):\n",
    "    for i in models:\n",
    "        def step(data):\n",
    "            model = i() \n",
    "            model.fit(data['train_data'], data['train_labels'])\n",
    "            model.fit(data['train_data'], data['train_labels'])\n",
    "            output_labels = model.predict(data['test_data'])\n",
    "            yield ('pred_true_values', (output_labels, data['test_labels']))\n",
    "        yield letp.Cycle(analyzer, data_handler, step, name=i.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run everything and collect the outputs, we can do the following. We're going to see some warnings, but we can silence them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "for i in cycle_generator(model_list, analyzer, data_handler):\n",
    "    results[i._name] = i.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print the collected results, this won't be pretty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was predictably ugly. Lets just mark down the accuracy, because that's usually the thing people are most interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in results:\n",
    "    for i in results[k]:\n",
    "        if 'analysis' in i:\n",
    "            print(f'{k} accuracy: {results[k][i][\"accuracy\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try with a different data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hopefully_this_good.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def k_fold_partitioner(data):\n",
    "    data_values = data[interesting_columns].values\n",
    "    data_labels = data[['SC_VIOLATION']].values\n",
    "    kfold = KFold(10)\n",
    "    for train, test in kfold.split(data_labels):\n",
    "        output = { \n",
    "            'train_data': data_values[train],\n",
    "            'train_labels': data_labels[train],\n",
    "            'test_data': data_values[test],\n",
    "            'test_labels': data_labels[test]\n",
    "        }\n",
    "        yield output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = letp.DataHandler(df, partitioner=kyra_partitioner)\n",
    "results = dict()\n",
    "for i in cycle_generator(model_list, analyzer, data_handler):\n",
    "    results[i._name] = i.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(['test', 'accuracy', 'recall', 'f1', 'precision'])\n",
    "for k in results:\n",
    "    running_accuracy = 0\n",
    "    running_recall = 0\n",
    "    running_f1 = 0\n",
    "    running_precision = 0\n",
    "    test_count = 0\n",
    "    for i in results[k]:\n",
    "        if 'analysis' in i:\n",
    "            test_count = test_count + 1\n",
    "            d[\"test\"]=i\n",
    "            d[\"accuracy\"]=results[k][i][\"accuracy\"]\n",
    "            d[\"f1\"]=results[k][i][\"f1\"]\n",
    "            d[\"precision\"]=results[k][i][\"precision\"]\n",
    "            running_accuracy += results[k][i][\"accuracy\"]\n",
    "            running_recall += results[k][i][\"recall\"]\n",
    "            running_f1 += results[k][i][\"f1\"]\n",
    "            running_precision += results[k][i][\"precision\"]\n",
    "    df[\"\"]\n",
    "    print(f'{k} accuracy: {running_accuracy/test_count:.2f}')\n",
    "    print(f'{k} recall: {running_recall/test_count:.2f}')\n",
    "    print(f'{k} f1: {running_f1/test_count:.2f}')\n",
    "    print(f'{k} precision: {running_precision/test_count:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
